{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook shows how to use the distributed environment in the `DistributedJobs` module, which leverages the base Julia `Distributed` package to run jobs across multiple processes. Each job is just a function that takes an `Array` input and produces an `Array` output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Environment Setup \n",
    "First define the number of processes to be used and include the required modules in each of the processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Int64}:\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the number of processes\n",
    "NUM_PROCS = 4\n",
    "\n",
    "# Set the number of processes to use for parallel computing\n",
    "using Distributed\n",
    "addprocs(NUM_PROCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add packages to all processes\n",
    "@everywhere include(\"../src/industrial_stats.jl\")\n",
    "@everywhere using .IndustrialStats: ModelBuilder, DesignInitializer, DistributedJobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Distributed Job\n",
    "Suppose I wish to run an optimization algorithm against $10,000$ randomly initialized designs on the simplex and I want to spread the computing out over all four of my available processes. To do this I can generate large batches of designs and then optimize the batches in parallel using different processess for each batch. If I choose a batch size of $500$, for example, then each process will handle $5=(10,000/(4\\cdot 500))$ total batches, or $2500$ design matrices.\n",
    "\n",
    "In addition to choosing the batch size, I must create a list of jobs to run. A job is a pair of functions $(f, g)$:\n",
    "- $f: \\mathbb R \\to \\mathbb R^{n \\times N_1\\times K_1}$ is a function that takes the batch size as input and produces a tensor of shape $n \\times N_1\\times K_1$, where $n$ is the batch size, $N_1$ is the first dim and $K_1$ the second.   \n",
    "- $g: \\mathbb R^{n \\times N_1\\times K_1} \\to \\mathbb R^{n \\times N_2\\times K_2}$ is a function that processes a batch and produces a new tensor possibly with a different shape\n",
    "\n",
    "Each job uses the generator function to create the input, then applies the processing function. In other words, the output is $g \\circ f$.\n",
    "\n",
    "These functions need to be defined in `@everywhere` blocks in order to run in multiple processes.\n",
    "\n",
    "For this example, I will use the `ModelBuilder` module in this repository to generate mixture designs for a first order Scheffe model, and an identity processing function; the actual $f$ and $g$ implementations can be anything as long as they operate on Julia `Array`s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Data Generator & Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere begin \n",
    "    # Returns a function that produces batch_size randomly initialized mixture designs\n",
    "    # This is the f batch generator function\n",
    "    function create_design_generator(N1, K1, model_builder; batch_size=50)\n",
    "        # Init is a function that produces (batch_size)xN1xK1 tensors\n",
    "        init = DesignInitializer.make_initializer(N1, K1, model_builder; type = \"mixture\")\n",
    "\n",
    "        # Each time the generator is invoked, it will in turn invoke the init function to create a new batch\n",
    "        return () -> init(batch_size)\n",
    "    end\n",
    "\n",
    "    # Simpler generator example for basic uniform samples \n",
    "    # Another example of f \n",
    "    function create_random_generator(N1, K1, batch_size)\n",
    "        return () => rand(batch_size, N1, K1)\n",
    "    end\n",
    "\n",
    "    # Stand-in optimizer function\n",
    "    # Replace with any function that accepts Array input\n",
    "    # This is a g batch processing function\n",
    "    function my_optimizer(data::Array)\n",
    "        return data\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create & Run Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mDistributing 20 jobs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      From worker 3:\t\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSaving job 'compute_job_3.0' to '../data/compute_job_3.0 2024-09-07 21:41:35.h5'\n",
      "      From worker 5:\t\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSaving job 'compute_job_1.0' to '../data/compute_job_1.0 2024-09-07 21:41:35.h5'\n",
      "      From worker 4:\t\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSaving job 'compute_job_4.0' to '../data/compute_job_4.0 2024-09-07 21:41:35.h5'\n",
      "      From worker 2:\t\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSaving job 'compute_job_2.0' to '../data/compute_job_2.0 2024-09-07 21:41:35.h5'\n",
      "      From worker 3:\t\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSaving job 'compute_job_7.0' to '../data/compute_job_7.0 2024-09-07 21:41:37.h5'\n",
      "      From worker 5:\t\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSaving job 'compute_job_5.0' to '../data/compute_job_5.0 2024-09-07 21:41:37.h5'\n",
      "      From worker 4:\t\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSaving job 'compute_job_6.0' to '../data/compute_job_6.0 2024-09-07 21:41:37.h5'\n",
      "      From worker 3:\t\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSaving job 'compute_job_8.0' to '../data/compute_job_8.0 2024-09-07 21:41:37.h5'\n",
      "      From worker 5:\t\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSaving job 'compute_job_10.0' to '../data/compute_job_10.0 2024-09-07 21:41:37.h5'\n",
      "      From worker 4:\t\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSaving job 'compute_job_9.0' to '../data/compute_job_9.0 2024-09-07 21:41:37.h5'\n",
      "      From worker 5:\t\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSaving job 'compute_job_12.0' to '../data/compute_job_12.0 2024-09-07 21:41:37.h5'\n",
      "      From worker 3:\t\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSaving job 'compute_job_11.0' to '../data/compute_job_11.0 2024-09-07 21:41:37.h5'\n",
      "      From worker 4:\t\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSaving job 'compute_job_13.0' to '../data/compute_job_13.0 2024-09-07 21:41:37.h5'\n",
      "      From worker 2:\t\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSaving job 'compute_job_17.0' to '../data/compute_job_17.0 2024-09-07 21:41:37.h5'\n",
      "      From worker 2:\t\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSaving job 'compute_job_18.0' to '../data/compute_job_18.0 2024-09-07 21:41:37.h5'\n",
      "      From worker 2:\t\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSaving job 'compute_job_19.0' to '../data/compute_job_19.0 2024-09-07 21:41:37.h5'\n",
      "      From worker 4:\t\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSaving job 'compute_job_15.0' to '../data/compute_job_15.0 2024-09-07 21:41:37.h5'\n",
      "      From worker 5:\t\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSaving job 'compute_job_14.0' to '../data/compute_job_14.0 2024-09-07 21:41:37.h5'\n",
      "      From worker 3:\t\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSaving job 'compute_job_16.0' to '../data/compute_job_16.0 2024-09-07 21:41:37.h5'\n",
      "      From worker 2:\t\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSaving job 'compute_job_20.0' to '../data/compute_job_20.0 2024-09-07 21:41:37.h5'\n",
      "\n",
      "\n",
      "Completed 20 jobs."
     ]
    }
   ],
   "source": [
    "# Configure experiment settings\n",
    "N = 7\n",
    "K = 3\n",
    "\n",
    "# Using the first-order scheffe implementation in the ModelBuilder module\n",
    "model_builder = ModelBuilder.scheffe(1)\n",
    "\n",
    "# Define batch size\n",
    "num_samples = 10_000\n",
    "batch_size = 500\n",
    "\n",
    "# Define data output location\n",
    "# Data is saved to the disk and can be loaded later on\n",
    "path_prefix = \"../data\"\n",
    "\n",
    "# Function that maps indices to Jobs\n",
    "job_creator = (idx) -> \n",
    "    DistributedJobs.create_job(\n",
    "        my_optimizer, # processing function\n",
    "        create_design_generator(N, K, model_builder; batch_size = 500); # tensor generating function\n",
    "        name = \"compute_job_$idx\" # Used to set the file name and the HDF5 dataset name \n",
    "    )\n",
    "\n",
    "# Create vector of jobs\n",
    "jobs = map(job_creator, 1:(num_samples / batch_size))\n",
    "\n",
    "# Run jobs\n",
    "results = DistributedJobs.run_jobs(jobs; path_prefix=path_prefix)\n",
    "\n",
    "print(\"\\n\\nCompleted $(length(results)) jobs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Analyze\n",
    "The stored data can be loaded for analysis using HDF5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 7, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = DistributedJobs.load_and_concatenate(results)\n",
    "size(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.5",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
